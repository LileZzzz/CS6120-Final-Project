{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKdqeUOLprIt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "from torchtext.vocab import vocab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset class which preprocess text data and converts it into input\n",
        "    tensors suitable for the models.\n",
        "    \"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, vocabulary=None):\n",
        "        self.tokenizer= tokenizer\n",
        "        self.tokenized_texts= [self.tokenizer(text) for text in texts]\n",
        "        self.labels= self._preprocess_labels(labels)\n",
        "\n",
        "        # Building a vocabulary if not present\n",
        "        if vocabulary is None:\n",
        "            counter= Counter(word for tokens in self.tokenized_texts for word in tokens)\n",
        "            self.vocabulary= vocab(counter, min_freq=1, specials=[\"<unk>\"])\n",
        "            self.vocabulary.set_default_index(self.vocabulary[\"<unk>\"])\n",
        "        else:\n",
        "            self.vocabulary= vocabulary\n",
        "\n",
        "        #Converting tokenized text into indices based on the vocabulary\n",
        "        self.numerical_texts= [[self.vocabulary[token] for token in tokens] for tokens in self.tokenized_texts]\n",
        "        self.inputs= pad_sequence(\n",
        "            [torch.tensor(seq) for seq in self.numerical_texts],\n",
        "            batch_first=True,\n",
        "            padding_value=0\n",
        "        ) #Padding to ensure uniform length of the input sequence\n",
        "\n",
        "    def _preprocess_labels(self, labels):\n",
        "        encoded_labels= np.zeros((len(labels), 28))\n",
        "\n",
        "        #Setting labels with 1 for the corresponding classes\n",
        "        for i, label_list in enumerate(labels):\n",
        "            for label in label_list:\n",
        "                encoded_labels[i][label]= 1\n",
        "\n",
        "        return torch.tensor(encoded_labels, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "fbnVT4Mpitvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A Convolutional Neural Network for the text classification task\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes, kernel_sizes, num_filters):\n",
        "        super(CNN, self).__init__()\n",
        "        self.embedding= nn.Embedding(vocab_size, embed_dim)\n",
        "        self.convs= nn.ModuleList([\n",
        "            nn.Conv2d(1, num_filters, (k, embed_dim)) for k in kernel_sizes\n",
        "        ])\n",
        "        self.fc= nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
        "        self.dropout= nn.Dropout(0.5)\n",
        "        self.sigmoid= nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.embedding(x)\n",
        "        x= x.unsqueeze(1)\n",
        "\n",
        "        #Applying convolutional layer and RELU activation\n",
        "        conv_outputs= [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "        #Applying max pool across the sequence length and concatenating pooled outputs\n",
        "        pooled_outputs= [torch.max(output, dim=2).values for output in conv_outputs]\n",
        "        x= torch.cat(pooled_outputs, dim=1)\n",
        "\n",
        "        x= self.dropout(x)\n",
        "        x= self.fc(x)\n",
        "\n",
        "        return self.sigmoid(x) #Returning sigmoid to get class probabilities\n"
      ],
      "metadata": {
        "id": "9n0Xly5jQLiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, device, num_epochs=5):\n",
        "  criterion= nn.BCELoss()\n",
        "  optimizer= optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  model.train()\n",
        "  for epoch in range(num_epochs):\n",
        "      epoch_loss= 0\n",
        "      for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs= model(inputs)\n",
        "\n",
        "          loss= criterion(outputs, labels)\n",
        "          epoch_loss += loss.item()\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "      print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "86o4I1_NQdvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader):\n",
        "    class_names = [\n",
        "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\",\n",
        "    \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\",\n",
        "    \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\",\n",
        "    \"love\", \"nervousness\", \"optimism\", \"pride\", \"realization\", \"relief\",\n",
        "    \"remorse\", \"sadness\", \"surprise\", \"neutral\"\n",
        "    ]\n",
        "    model.eval()\n",
        "    all_predictions= []\n",
        "    all_labels= []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            inputs, labels= inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs= model(inputs)\n",
        "            predictions= (outputs > 0.5).int() #Applying threshold on probabilites\n",
        "            all_predictions.append(predictions.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "\n",
        "    #Concatenating all batch prediction and labels into single tensors\n",
        "    all_predictions= torch.cat(all_predictions, dim=0).numpy()\n",
        "    all_labels= torch.cat(all_labels, dim=0).numpy()\n",
        "\n",
        "    #Returning a classification report\n",
        "    return classification_report(all_labels, all_predictions, target_names=class_names, zero_division=0)"
      ],
      "metadata": {
        "id": "PI5muow4ScnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the dataset\n",
        "dataset= load_dataset(\"google-research-datasets/go_emotions\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "JOwk8NnYKOS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df= pd.DataFrame(dataset[\"train\"])\n",
        "test_df= pd.DataFrame(dataset[\"test\"])"
      ],
      "metadata": {
        "id": "46ajuu2AKQ7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting labels into integers to prevent problems dring preprocessing\n",
        "train_df['labels']= train_df['labels'].apply(lambda x: [int(label) for label in x])\n",
        "test_df['labels']= test_df['labels'].apply(lambda x: [int(label) for label in x])"
      ],
      "metadata": {
        "id": "nPzCJy5OGvSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting a basic english tokenizer\n",
        "tokenizer= get_tokenizer(\"basic_english\")"
      ],
      "metadata": {
        "id": "6Ejz-YCVmt4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing the datasets\n",
        "train_dataset= EmotionDataset(train_df[\"text\"], train_df[\"labels\"], tokenizer= tokenizer)\n",
        "test_dataset= EmotionDataset(test_df[\"text\"], test_df[\"labels\"], vocabulary=train_dataset.vocabulary, tokenizer= tokenizer)"
      ],
      "metadata": {
        "id": "qtk82B1VmpYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Batching the data\n",
        "train_dataloader= DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "test_dataloader= DataLoader(test_dataset, batch_size=2, shuffle=False)"
      ],
      "metadata": {
        "id": "r1qL5l-fiTMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting hyperparameters for the model (Achieved best results with these results)\n",
        "vocab_size= len(train_dataset.vocabulary)\n",
        "embed_dim= 50\n",
        "num_classes= 28\n",
        "kernel_sizes= [3, 4, 5]\n",
        "num_filters= 100\n",
        "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_epochs= 5\n",
        "\n",
        "model= CNN(vocab_size, embed_dim, num_classes, kernel_sizes, num_filters).to(device)"
      ],
      "metadata": {
        "id": "XEvHsrH6jpx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "train_model(model, train_dataloader, device)"
      ],
      "metadata": {
        "id": "osVJJHRvkEfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating a classification report for the model performance\n",
        "report= evaluate(model, test_dataloader)\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "vjavAyVFk5FR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}