{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ooQf6XwwJKMt",
    "outputId": "b8aa5fa4-6788-402b-db6b-a701d6066f04"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5HpWBOodI_P4",
    "outputId": "8cffc94c-bdee-4740-e4f1-f244c1cf20a4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, Features, Sequence, Value\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.special import expit\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "dataset = load_dataset(\"go_emotions\")\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# Use HF token\n",
    "HF_TOKEN = \"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=HF_TOKEN)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=28,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "print(dataset)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdiDTv_4I_P6"
   },
   "outputs": [],
   "source": [
    "# Tokenize and one-hot encode labels\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "\n",
    "    # One-hot encode the labels\n",
    "    num_classes = 28\n",
    "    one_hot_labels = np.zeros((len(examples[\"labels\"]), num_classes), dtype=np.float32)\n",
    "    for i, labels in enumerate(examples[\"labels\"]):\n",
    "        one_hot_labels[i, labels] = 1.0\n",
    "\n",
    "    tokenized[\"labels\"] = one_hot_labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zj1JlFSJI_P7"
   },
   "outputs": [],
   "source": [
    "new_features = Features(\n",
    "    {\n",
    "        \"text\": Value(\"string\"),\n",
    "        \"labels\": Sequence(Value(\"float32\")),\n",
    "        \"id\": Value(\"string\"),\n",
    "        \"input_ids\": Sequence(Value(\"int32\")),\n",
    "        \"attention_mask\": Sequence(Value(\"int32\")),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Cast the dataset to the new type\n",
    "tokenized_dataset = tokenized_dataset.cast(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1qHjmZpI_P7",
    "outputId": "f515eef3-1dab-454f-dd75-57700665e1ae"
   },
   "outputs": [],
   "source": [
    "# Explore the train and validation loss under different batch sizes, learning rates and epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "J4Bf7M2SI_P7",
    "outputId": "0bcb98d7-cbda-4b46-feca-d86f0ced1145"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYxsRIgjI_P7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the training history\n",
    "# Extract logs from Trainer's state\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Separate training and validation losses and corresponding steps\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "steps = []\n",
    "\n",
    "for log in log_history:\n",
    "    if \"loss\" in log:  # Training loss\n",
    "        train_losses.append(log[\"loss\"])\n",
    "        steps.append(log[\"step\"])\n",
    "    if \"eval_loss\" in log:  # Validation loss\n",
    "        eval_losses.append(log[\"eval_loss\"])\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, train_losses, label=\"Training Loss\", marker=\"o\")\n",
    "if eval_losses:  # Only plot eval loss if it's available\n",
    "    plt.plot(steps[:len(eval_losses)], eval_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bY1a0r-WI_P8"
   },
   "outputs": [],
   "source": [
    "# Generate a detailed classification report for the test set\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPlDrJtTI_P8"
   },
   "outputs": [],
   "source": [
    "# Extract logits and labels\n",
    "logits = predictions.predictions[0]\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Apply sigmoid to get probabilities\n",
    "probabilities = expit(logits)\n",
    "\n",
    "# Apply threshold to probabilities for multilabel classification\n",
    "threshold = 0.5\n",
    "predicted_labels = (probabilities > threshold).astype(int)\n",
    "\n",
    "# Generate the classification report\n",
    "print(\"Test Set Accuracy:\", accuracy_score(labels, predicted_labels))\n",
    "report = classification_report(labels, predicted_labels, output_dict=False, zero_division=0)\n",
    "print(\"Detailed Classification Report:\\n\", report)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
